# -*- coding: utf-8 -*-
"""CNN_fake_news1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ska9E_6zY7IuakBIUkfSMpg8QuC0EekY
"""

import tensorflow as tf
import keras._tf_keras.keras
from keras._tf_keras.keras import models, layers, datasets
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print(tf.__version__)

news_1=pd.read_csv("/IFND_2.csv")

news_1.head()

news_1.isnull().sum()

news_1=news_1.fillna('')

news_1['bin_label'] = news_1['Label'].map({'Fake': 0, 'TRUE': 1})

news_1.drop(columns=['Image', 'id','Category','Date','Label'], axis=1, inplace=True)

import nltk
nltk.download('stopwords')
from nltk.stem.porter import PorterStemmer
port_stem = PorterStemmer()
def stemming(content):
    content=str(content)
    stemmed_content=re.sub('[^a-zA-Z]',' ', content)
    stemmed_content=stemmed_content.lower()
    stemmed_content=stemmed_content.split()
    stemmed_content=[port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content=' '.join(stemmed_content)
    return stemmed_content

import re
from nltk.corpus import stopwords
news_1['Statement']=news_1['Statement'].apply(stemming)

news_1['content']=news_1['Statement']+" "+news_1['Web']

news_1.head()

news_1.drop(columns=['Statement','Web'], axis=1, inplace=True)

news_1['content_length'] = news_1['content'].str.len()

# Get the maximum length
max_length = news_1['content_length'].max()

# Find the entry with the maximum length
longest_entry = news_1.loc[news_1['content_length'] == max_length, 'content'].iloc[0]

print(f"Maximum length: {max_length}")
print(f"\nLongest entry:\n{longest_entry}")

news_1.drop(columns=['content_length'], axis=1, inplace=True)

train_data, test_data = train_test_split(news_1, test_size=0.2, random_state=42)

Y_train=train_data['bin_label']
Y_test=test_data['bin_label']

from keras._tf_keras.keras.preprocessing.text import Tokenizer
from keras._tf_keras.keras.preprocessing.sequence import pad_sequences
tokenizer=Tokenizer()
tokenizer.fit_on_texts(train_data['content'])
X_train=pad_sequences(tokenizer.texts_to_sequences(train_data['content']),maxlen=250)
X_test=pad_sequences(tokenizer.texts_to_sequences(test_data['content']),maxlen=250)

# Check the maximum token ID in your data
print("Max token ID in X_train:", X_train.max())
print("Shape of X_train:", X_train.shape)

from collections import Counter
import re
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming train_data has 'Statement' and a column indicating fake/real (let's call it 'Label')
# where 1 might be fake and 0 might be real (adjust according to your dataset)

# Function to clean text
def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters, numbers, etc.
    text = re.sub(r'[^\w\s]', '', text)
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # Split into words
    words = text.split()
    # Optional: Remove stop words
    # from nltk.corpus import stopwords
    # stop_words = set(stopwords.words('english'))
    # words = [word for word in words if word not in stop_words]
    return words

# Separate fake and real news
fake_news = train_data[train_data['bin_label'] == 1]['content'].tolist()
real_news = train_data[train_data['bin_label'] == 0]['content'].tolist()

# Process and count words
fake_words = []
for text in fake_news:
    fake_words.extend(clean_text(text))

real_words = []
for text in real_news:
    real_words.extend(clean_text(text))

# Count frequencies
fake_counter = Counter(fake_words)
real_counter = Counter(real_words)

# Get top words
top_n = 20
top_fake_words = fake_counter.most_common(top_n)
top_real_words = real_counter.most_common(top_n)

print("Top words in fake news:")
for word, count in top_fake_words:
    print(f"{word}: {count}")

print("\nTop words in real news:")
for word, count in top_real_words:
    print(f"{word}: {count}")

# Optional: Visualize the results
plt.figure(figsize=(12, 10))

plt.subplot(2, 1, 1)
sns.barplot(x=[word for word, _ in top_fake_words], y=[count for _, count in top_fake_words])
plt.title('Top words in Fake News')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Count')

plt.subplot(2, 1, 2)
sns.barplot(x=[word for word, _ in top_real_words], y=[count for _, count in top_real_words])
plt.title('Top words in Real News')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

news_1.head

cnn=models.Sequential([
    layers.Embedding(11690,128,input_length=250),
    layers.Conv1D(250,kernel_size=(3),padding='valid',activation='relu',strides=1),
    layers.MaxPooling1D(pool_size=5),
    layers.Flatten(),
    layers.Dense(40,activation='relu'),
    layers.Dense(10,activation='relu'),
    layers.Dense(1,activation='sigmoid')
])
cnn.build((250,250))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from keras._tf_keras.keras import metrics
cnn.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        metrics.Recall(name='recall'),
        metrics.Precision(name='precision'),
        metrics.F1Score(name='f1_score')
    ]
)

from keras._tf_keras.keras.callbacks import EarlyStopping
callback =  EarlyStopping(
    monitor="val_loss",
    min_delta=0.00001,
    patience=5,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=False
)

cnn.summary()

cnn.fit(X_train, Y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=callback)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# Predicting the probabilities for test data
y_pred_prob = cnn.predict(X_test)

# Converting the probabilities to binary classes based on threshold
y_pred = (y_pred_prob > 0.5).astype(int)

# Calculating the evaluation metrics
accuracy = accuracy_score(Y_test, y_pred)
precision = precision_score(Y_test, y_pred)
recall = recall_score(Y_test, y_pred)
f1 = f1_score(Y_test, y_pred)

# Printing the evaluation metrics
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1-score:', f1)

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
y_pred=cnn.predict(X_test)
# y_pred_classes=[np.argmax(element) for element in y_pred]
y_pred_classes=(y_pred>0.75).astype(int)
print("Classification report: \n",classification_report(Y_test,y_pred_classes))

# print(y_pred_classes)

y_pred.shape

print(Y_test)
Y_test.shape

print(y_pred)

cnn.get_weights()

weights = cnn.get_weights()
shapes = [w.shape for w in weights]
print(shapes)
print(len(weights))

def predict_news1(news):
    sequences = tokenizer.texts_to_sequences([news])
    padded_sequence =pad_sequences(
        sequences, maxlen=250)

    # Get prediction
    prediction = cnn.predict(padded_sequence)

    # Apply threshold for classification
    result = "most probably true" if prediction[0][0] > 0.5 else "most probably false"

    # For debugging
    print(f"Raw prediction value: {prediction[0][0]}")

    return result

# example usage
news = "Philip Cheung Cats can read your mind and are plotting against you"
predict = predict_news1(news)
print(f"This News is {predict}")

news = "Mumbai Man Claims to Have Found 'Water' on Mars Using His Smartphone"
predict = predict_news1(news)
print(f"This News is {predict}")

news = "new species of fish discovered in the Amazon rainforest"
predict = predict_news1(news)
print(f"This News is {predict}")

news = "Dr. Anna Lee Scientists Warn of New 'Super Mosquito' That Buzzes Loud Enough to Ruin Any Barbecue"
predict = predict_news1(news)
print(f"This News is {predict}")

news = "Neeraj Chopra won gold medal in javelin throw in Japan Olympics"
predict = predict_news1(news)
print(f"This News is {predict}")

news = "The Indian government is providing free â‚¹10,000 under a new scheme."
predict = predict_news1(news)
print(f"This News is {predict}")

news = "Trump Pauses U.S. Immigration Program For Ukrainians; Nearly 300,000 People At Risk"
predict = predict_news1(news)
print(f"This News is {predict}")

news = "trump imposes tariff on china"
predict = predict_news1(news)
print(f"This News is {predict}")

news = "Scientists Invent New Color That Instantly Makes People Confused"
predict = predict_news(news)
print(f"This News is {predict}")

news = "IndusInd Bank, India's fifth-largest private-sector lender, has disclosed major accounting errors, leading to a 25% drop in its market value and raising concerns about governance in the banking sector."
predict = predict_news1(news)
print(f"This News is {predict}")

"""Saving trained model"""

import pickle

with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

cnn.save('cnn_news_model_2.h5')

def predict_news1(news):
  # tokenize and pad the review
  sequence = tokenizer.texts_to_sequences([news])
  padded_sequence = pad_sequences(sequence, maxlen=200)
  prediction = loaded_model.predict(padded_sequence)
  predict = "most probably true" if prediction[0][0] > 0.5 else "most probably false"
  return predict

news = "IndusInd Bank, India's fifth-largest private-sector lender, has disclosed major accounting errors, leading to a 25% drop in its market value and raising concerns about governance in the banking sector."
predict = predict_news1(news)
print(f"This News is {predict}")

